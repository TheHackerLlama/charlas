{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "debugging_transformers.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUjht6MkRGUQsEE8qRj9NN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheHackerLlama/charlas/blob/main/debugging_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JkPFRtFJDBV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "id": "h5S2I4MAK1FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "2wXtGaX7JF0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name\n",
        "\n",
        "def copy_repository_template():\n",
        "    # Clone the repo and extract the local path\n",
        "    template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\"\n",
        "    commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\"\n",
        "    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)\n",
        "    # Create an empty repo on the Hub\n",
        "    model_name = template_repo_id.split(\"/\")[1]\n",
        "    create_repo(model_name, exist_ok=True)\n",
        "    # Clone the empty repo\n",
        "    new_repo_id = get_full_repo_name(model_name)\n",
        "    new_repo_dir = model_name\n",
        "    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)\n",
        "    # Copy files\n",
        "    copy_tree(template_repo_dir, new_repo_dir)\n",
        "    # Push to Hub\n",
        "    repo.push_to_hub()"
      ],
      "metadata": {
        "id": "gy2c_m6iJNI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_repository_template()"
      ],
      "metadata": {
        "id": "070YXOnmKo0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging the pipeline"
      ],
      "metadata": {
        "id": "2rJBYhGWJXXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\")"
      ],
      "metadata": {
        "id": "dMd0EwQnJV7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Omar and I work at Hugging Face in a mountain\"\n",
        ")"
      ],
      "metadata": {
        "id": "869W7R8UJaPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\")\n",
        "reader = pipeline(\"question-answering\", model=model_checkpoint)"
      ],
      "metadata": {
        "id": "tNwNcSJKJvbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\")\n",
        "reader = pipeline(\"question-answering\", model=model_checkpoint)"
      ],
      "metadata": {
        "id": "xrnf25ohJ-tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "pretrained_checkpoint = \"distilbert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(pretrained_checkpoint)"
      ],
      "metadata": {
        "id": "YsLCfTEzKbng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.push_to_hub(model_checkpoint, commit_message=\"Add config.json\")"
      ],
      "metadata": {
        "id": "8U3NFrW3UGFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\")\n",
        "\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text\n",
        "given a question. An example of a question answering dataset is the SQuAD\n",
        "dataset, which is entirely based on that task. If you would like to fine-tune a\n",
        "model on a SQuAD task, you may leverage the\n",
        "examples/pytorch/question-answering/run_squad.py script.\n",
        "\n",
        "ðŸ¤— Transformers is interoperable with the PyTorch, TensorFlow, and JAX\n",
        "frameworks, so you can use your favourite tools for a wide variety of tasks!\n",
        "\"\"\"\n",
        "\n",
        "question = \"What is extractive question answering?\"\n",
        "reader(question=question, context=context)"
      ],
      "metadata": {
        "id": "0mcKRrqVUJAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging de forward pass"
      ],
      "metadata": {
        "id": "i3SL2V1ZUhfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = reader.tokenizer\n",
        "model = reader.model"
      ],
      "metadata": {
        "id": "sGlFXlbmURm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which frameworks can I use?\""
      ],
      "metadata": {
        "id": "heGkC6WDUmFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "pt-5X0kOUnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos las entradas\n",
        "inputs = tokenizer(question, context, add_special_tokens=True)\n",
        "\n",
        "# Hacemos inferencia\n",
        "outputs = model(**inputs)\n",
        "\n",
        "answer_start_scores = outputs.start_logits\n",
        "answer_end_scores = outputs.end_logits\n",
        "\n",
        "# Get the most likely beginning of answer with the argmax of the score\n",
        "answer_start = torch.argmax(answer_start_scores)\n",
        "\n",
        "# Get the most likely end of answer with the argmax of the score\n",
        "answer_end = torch.argmax(answer_end_scores) + 1\n",
        "input_ids = inputs[\"input_ids\"][0]\n",
        "answer = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        ")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "SOJtu_EQUr4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"][:5]"
      ],
      "metadata": {
        "id": "TfZ4sCTyUxuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "hcvHBif0VJRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizamos las entradas\n",
        "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "# Hacemos inferencia\n",
        "outputs = model(**inputs)\n",
        "\n",
        "answer_start_scores = outputs.start_logits\n",
        "answer_end_scores = outputs.end_logits\n",
        "\n",
        "# Get the most likely beginning of answer with the argmax of the score\n",
        "answer_start = torch.argmax(answer_start_scores)\n",
        "\n",
        "# Get the most likely end of answer with the argmax of the score\n",
        "answer_end = torch.argmax(answer_end_scores) + 1\n",
        "input_ids = inputs[\"input_ids\"][0]\n",
        "answer = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        ")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "MjMQwTx6VK0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}