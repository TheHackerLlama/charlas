{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parte_3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzlV2XSdsSUmy4sSmzg/IE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheHackerLlama/charlas/blob/main/riiaa_2021/parte_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ11Og6YPHZP"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers datasets\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+${CUDA}.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcC70tvMPRpe"
      },
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image = Image.open(\"doge.png\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z7yVMGB7dPW"
      },
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "model_ckpt = 'google/vit-base-patch16-224'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_ckpt)\n",
        "model = ViTForImageClassification.from_pretrained(model_ckpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We2LjXLY73MP"
      },
      "source": [
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snr3BvZI77ZQ"
      },
      "source": [
        "book_data = [\n",
        "    {'chapter': 0,  'name': 'Introduction', 'start_page': 1, 'end_page': 11},\n",
        "    {'chapter': 1,  'name': 'Text classification', 'start_page': 12, 'end_page': 48},\n",
        "    {'chapter': 2,  'name': 'Named Entity Recognition', 'start_page': 49, 'end_page': 73},\n",
        "    {'chapter': 3,  'name': 'Question Answering', 'start_page': 74, 'end_page': 120},\n",
        "    {'chapter': 4,  'name': 'Summarization', 'start_page': 121, 'end_page': 140},\n",
        "    {'chapter': 5,  'name': 'Conclusion', 'start_page': 141, 'end_page': 144},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGsX7oCA8GPt"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "table = pd.DataFrame.from_records(book_data)\n",
        "table['number_of_pages'] = table['end_page']-table['start_page']\n",
        "table = table.astype(str)\n",
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrUd0QCM8SlK"
      },
      "source": [
        "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
        "\n",
        "model_name = 'google/tapas-base-finetuned-wtq'\n",
        "model = TapasForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = TapasTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXXJdNyS8Uvz"
      },
      "source": [
        "queries = [\"What's the topic in chapter 4?\",\n",
        "           \"What is the total number of pages?\",\n",
        "           \"On which page does the chapter about question-answering start?\",\n",
        "           \"How many chapters have more than 20 pages?\"]\n",
        "\n",
        "inputs = tokenizer(table=table, queries=queries, padding='max_length',\n",
        "                   return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "answer_coordinates, agg_indices = tokenizer.convert_logits_to_predictions(\n",
        "        inputs,\n",
        "        outputs.logits.detach(),\n",
        "        outputs.logits_aggregation.detach())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWdD_pW88WS7"
      },
      "source": [
        "# let's print out the results:\n",
        "id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3:\"COUNT\"}\n",
        "agg_string = [id2aggregation[x] for x in agg_indices]\n",
        "\n",
        "answers = []\n",
        "for coordinates in answer_coordinates:\n",
        "    if len(coordinates) == 1: # only a single cell:\n",
        "        answers.append(table.iat[coordinates[0]])\n",
        "    else: # multiple cells\n",
        "        cell_values = []\n",
        "        for coordinate in coordinates:\n",
        "            cell_values.append(table.iat[coordinate])\n",
        "        answers.append(\", \".join(cell_values))\n",
        "\n",
        "for query, answer, predicted_agg in zip(queries, answers, agg_string):\n",
        "    print(query)\n",
        "    if predicted_agg == \"NONE\": print(\"Predicted answer: \" + answer)\n",
        "    else: print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)\n",
        "    print('='*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkhipP_YC-JO"
      },
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTKAdLUVDI7t"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\",\n",
        "                  split=\"validation\")\n",
        "ds[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEQ4Kb1DiiG"
      },
      "source": [
        "import soundfile as sf\n",
        "\n",
        "def map_to_array(batch):\n",
        "    speech, _ = sf.read(batch[\"file\"])\n",
        "    batch[\"speech\"] = speech\n",
        "    return batch\n",
        "\n",
        "ds = ds.map(map_to_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Jk2gsnDKhj"
      },
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "display(Audio(ds[0]['speech'], rate=16000))\n",
        "display(Audio(ds[1]['speech'], rate=16000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF-ie9w7DOO8"
      },
      "source": [
        "import torch\n",
        "\n",
        "inputs = processor(ds[\"speech\"][:2], return_tensors=\"pt\", padding=\"longest\",\n",
        "                   sampling_rate=16000)\n",
        "logits = model(inputs.input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "print('\\n\\n'.join(transcription))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIGrvchbDoSb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}